{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c039b60-3400-4ba5-bf3d-30fe42c63091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prateekagarwal/miniconda3/envs/testre/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from FigCapsHF import FigCapsHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9c375e-bcea-4117-987f-5cbaf6271b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Object\n",
    "FigCapsHF = FigCapsHF(\"/Users/prateekagarwal/Downloads/benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bdb2f-66f7-4446-af46-aa59d9fcf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want to quickly train a BLIP model with RLHF while choosing a human feedback factor (TODO)\n",
    "python train_blip.py --mixed_precision fp16 --hf_score_type helpfulness --benchmark_path XX/benchmark\n",
    "\n",
    "#Want to run some inference\n",
    "python inference.py --figure_path /Users/prateekagarwal/Desktop/test_image.png --model_path /Users/prateekagarwal/Desktop/Model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bdc5ae2-a897-4d89-b5a5-9c1038f41277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The associated caption for this image is: distance error of inertial mechanisms with kalman filtering , as a function of the gnss unavailability period .\n"
     ]
    }
   ],
   "source": [
    "#Want to visualize example from dataset\n",
    "FigCapsHF.get_image_caption_pair(data_split = \"train\", image_name = \"1001.0025v1-Figure5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76f7b99-ed4b-4317-bb36-4dfc34c41709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The associated caption for this image is: results for impact of cost-sensitive learning . “ att ” means the loss function of variant-2 ; “ att+nr ” means only considering the cost of nr controlled by λ ignoring the cost controlled by γ based on variant-2 and λ is set to 0 ; “ att+p ” means considering the cost controlled by γ based on variant-2 ignoring the cost of nr and γ is set to 1 ; “ att+nr+p ” is the loss function of variant-3 and jointly considers the two kinds of costs mentioned above , λ is set to 0 and γ is 1 .\n",
      "\n",
      "The associated human feedback for the figure-caption pair is: helpfulness: 4.0, ocr: 5.0, visual: 2.0, takeaway: 2.0\n"
     ]
    }
   ],
   "source": [
    "#Want to visualize an example from the human annotated dataset and its associated annotations\n",
    "FigCapsHF.get_image_caption_pair_hf(image_name = \"1907.11521v1-Figure6-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3ec397-9be5-41fc-89ee-cd7457d9c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|█████████| 440M/440M [00:11<00:00, 37.0MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████| 399/399 [00:14<00:00, 27.87it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 28.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#Want to generate dataset like the experiment \n",
    "#embedding_model can be 'BERT', 'SciBERT', 'MCSE', or 'BLIP'\n",
    "#hf_score_type can be 'helpfulness','ocr','takeaway' or 'visual'\n",
    "inferred_hf_df = FigCapsHF.infer_hf_training_set(hf_score_type = \"helpfulness\", embedding_model = \"BERT\", max_num_samples = 100, quantization_levels = 3, mapped_hf_labels = [\"Bad\", \"Neutral\", \"Good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81da7f2-d208-4006-9f83-2d9395c3702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████| 399/399 [00:16<00:00, 24.84it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#Want to score a single user specified figure-caption pair \n",
    "hf_ds_embeddings, scores = FigCapsHF.generate_embeddings_hf_anno(hf_score_type = \"helpfulness\", embedding_model = \"BERT\")\n",
    "scoring_model = FigCapsHF.train_scoring_model(hf_ds_embeddings, scores)\n",
    "\n",
    "image_path = \"/Users/prateekagarwal/Desktop/Benchmark/No-Subfig-Img/train/1907.11521v1-Figure6-1.png\"\n",
    "caption = \"the graph indicates the loss of the model over successive generations\"\n",
    "\n",
    "embedding = FigCapsHF.generate_embeddings([image_path], [caption], embedding_model = \"BERT\")\n",
    "inferred_hf_score = scoring_model.predict(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:testre]",
   "language": "python",
   "name": "conda-env-testre-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
